\subsection{A.1: Variations}
\subsubsection{Variation 0: Baseline}

\subsubsection{Variation 1: Low-discrepency sampler}
Here we implement a discretised version of the low-discrepency sampler (LDS) from \cite{kingma2023variationaldiffusionmodels}.

The baseline loss function provided to us works by independently sampling a timestep $t \in \mathcal{U{0, \ldots, T}}$ for each image in the batch of size $k$.
 
The LDS loss function works by sampling a single offset $u_0 \sim \mathcal{U}[0,1]$.
Each timestep $t_i$ is then computed as $t_i = mod(u_0 + i/k, 1)$. Here 0 is the first timestep and 1 is the last timestep in the continuous definition of the DDPM.

To discretise this process, we multiply these values by $T$ to map from $[0,1]$ to $[0, T]$. Where $T$ is the total number of diffusions performed by the model, then round to the nearest integer.
We note that rounding is equivalent to taking the floor of the value offset by $+1/2$.

\begin{align}
  t_i &= \left\lfloor mod(u_0 + i/k, 1) \times T + \frac{1}{2} \right\rfloor, i = \{1, ..., k\}
\intertext{}
\end{align}


\subsubsection{Variation 2: Importance sampling}
To reduce the variance of the gradients when using the ELBO-bound loss, as discussed in "Improved Denoising Diffusion Probabilistic Models". 
To implement this we keep track of the previous 10 losses for each $t \in \{0,...,T\}$ And use an importance sampler to sample the time-step t from.
$$L_{vlb}= E_{t\sim p_t} \Left[\frac{L_t}{p_t}]\Right \;\; \text{with} \;\; p_t \probto \sqrt{E[L^2]}, \sum p_t = 1 $$.
We initialize training by running the model as the base model (sampling each t uniformly) until we have 10 samples for each time-step.
Once we have all of these samples we employ the importance sampler to sample new time-steps.
The initialization phase requires a large number of samples but is typically finished within the first iteration, even for $T=1000$
\subsubsection{Variation 4: Continuous-time diffusion model using SDEs}

\subsubsection{Variation 3: Classifier-free guidance}
Another variation we implemented is classifier-free guidance.
The motivation behind this method is two-fold:
\begin{enumerate}
  \item It allows us to generate specific images by providing additional guidance, in our case, the digit number.
  \item It allows the model to learn the conditional distributions, leading to better quality but less variability.
\end{enumerate}



To implement classifier-free guidance, we create a linear combination of a conditional and unconditional score estimator, as derived in \cite{classifier_free_guidance}, for our data which is conditioned on the labels
$y$.
\begin{align}
  \hat{\epsilon_\theta}(x_t, t, y) &= \epsilon_\theta(x_t, t, y) + w (\epsilon_\theta(x_t, t, y) - \epsilon_\theta(x_t, t))\label{cfg_epsilon}
\end{align}
where $w$ is a weighting factor that determines how strongly guidance is applied, at 0 it is unconditional, and as $w$ increases, the model becomes more guided,
with 1 leading to a conditional score estimator and $w > 1$ leading to increasingly higher fidelity and less diversity.
This estimator can be derived by looking at the conditional score $\nabla_{x_t} \log (p(y \mid x_t))$
and by applying Bayes' rule:
\begin{align}
  \nabla_{x_t} \log (p(y|x_t)) &= \nabla_{x_t} \log (p(x_t \mid y)) - \nabla_{x_t} \log (p(x_t)) + \nabla_{x_t} \log (p(y))\\
  \intertext{The last term is dropped since $y$ is constant with respect to $x_t$.}
  &= \nabla_{x_t} \log (p(x_t \mid y)) - \nabla_{x_t} \log (p(x_t))
\end{align}
With further derivation (using the equivalence between $\nabla_{x_t} \log (p(x_t))$ and $\epsilon_\theta(x_t, t)$, and the equation derived for classifier guided diffusion \cite{classifier_free_guidance}), the resulting equation \ref{cfg_epsilon} is reached. 

To implement this, we simply replace the original noise term $\epsilon_\theta(x_t, t)$ with the new term $\hat{\epsilon_\theta}(x_t, t, y)$.
We inject the class information into this new model $\hat{\epsilon_\theta}(x_t, t, y)$, giving us 
the conditioned model.
In the same way as we inject the time information into the model, we also inject the class information by adding a class embedding
which is analogous to the time embedding.
To get our unconditioned model, we mask 20\% of the class labels in training at random.
The masked value in our specific implementation is set to $10$, since 0 throgh 9 are 
the classes in the MNIST dataset.
We therefore get the unconditioned model $\epsilon_\theta(x_t, t) = \epsilon_\theta(x_t, t, y=10)$.

This method of dropping out the class information at random gives us a model
that both learns the conditional distributions of each digit, as well as the
general unconditional distribution of the whole dataset.

%We also have form \cite{anderson_reverse-time_1982} that any SDE has a reverse time SDE of the form:
%\begin{equation}
%  \frac{d x} =  [f(x, t)-g^2 (t) \nabla_x \log p_t (x)]dt + g(t)dW
%\end{equation}

This variation uses a stochastic differential equation to model the forward and reverse diffusion process. 
Our forward process is a variance exploding SDE of form:
\begin{equation}
  dx = \sigma ^ t dW, t \in [0, 1]
\end{equation}
Which gives us the weighting of
$\lambda(t) = \frac{1}{2 \log \sigma} (\sigma^{2t}-1)$ as suggested by \cite{yang_song}.
This function is chosen to be inversely proportional to $\mathbb{E}[||\nabla_x \log p_{0t}(x(t) | x(0))||^2_2]$.
This function was then used to replace the weighting of our score-net
which was set to 1 for all $t \in [0, 1]$ for the three other variations (since they predict the noise instead of the score).
Our loss function was defined to be the Fisher divergence between $s_\theta(x_t, t)$ and $\nabla_x \log p_{\text{data}}(x)$:
\begin{align}
  \mathcal{L}_\text{SDE}(\theta) &= \mathbb{E}_{x(0) \sim p_\text{data}} \mathbb{E}_{t \sim \mathcal{U}[0, 1]} \left[ 
  \lambda(t) \mathbb{E}_{x(t) \sim q_{\sigma_t}(x(t) | x(0))} \left[
    ||s_\theta (x_t, t) - \nabla_{x(t)} (\log q(x(t) | x(0)))||^2_2
    \right]\right]\\
  \intertext{Furthermore, we also normalise the score-net
  inversely proportional to the marginal distribution, so we can simply the loss function:}
    &\approx \mathbb{E}_{x(0) \sim p_\text{data}} \mathbb{E}_{t \sim \mathcal{U}[0, 1]} \left[ 
  \lambda(t) \mathbb{E}_{x(t) \sim q_{\sigma_t}(x(t) | x(0))} \left[
    \left|\left|\frac{s_\theta(x_t, t)}{\lambda(t)}\right|\right|^2_2
    \right]\right]
\end{align}
We then train the model and perform our sampling using Euler-Maruyama sampling.

\subsection{A.2 Comparison}

\subsubsection{Visual Comparison}
Each of the models were trained for 100 epochs, then 10 sample images were created for each model, as can be seen in~\cref{fig:ddpm:gensamples}.
Comparing these results visually we determined that the conditional diffusion produces the best digits (which is somewhat excepted since we are able to condition on digit classes when generating samples).
We also conclude that the model using importance sampling produced the worst digits. 
The other methods seem to have similar performance, but it is interesting to note that although the digits produced by SDE have high readability, they differ much from the other methods, in the sense that they are slimmer.


\subsubsection{Quatitative Comparison}
For our qualitative Comparison we consider, loss (negative log-likelihood), FID-score and Inception-score, to get a full picture.

\begin{itemize}
    \item likelihood\\
    The main issue when considering the likelihood of the different methods is that likelihood doesn't indicate generalization (a model that overfits will archive a great likelihood).
    computing the exact likelihood is intractable, so we have to depend on estimates such as the ELBO-bound. Another issue is that the likelihood is model dependent which means that our SDE isn't comparable to the other methods since we are using a different normalization.\\
    Looking at the Results in~\cref{fig:ddpm:loss} 
    We can notice that the loss curves are very similar.
    The main takeaway is that our base model performs as good or better than the more advanced methods under training.
    Furthermore we can notice that the Importance sampling method trains faster than the LDS method in the beginning but tapers of sooner. 
     
    \item Inception-score\\
    The inceptions-score uses the pre-trained Inception v3 to quantify the quality of images produced by model.
    High inception score is good, and indicates low entropy and diverse generated samples.
    Running the Inception v3 locally required large amounts of computes, so we estimated it using only 100 samples.
    Since these estimates are very noisy we used a 20 epoch moving average for evaluation.
    Looking at the Results in~\cref{fig:ddpm:inception}
    We notice that the SDE method starts of with the highest Inception score but that it stagnates quickly, meaning that all of the other methods outperformed it halfway through training.
    As for the other methods performance is similar but we can notice that the Conditional method improves than the Importance sampling method, with the tradeoff being that it stagnates earlier on.   
    
    \item FID-score\\
    The fid-score is based on the Inception-score, but also considers the ground truth of the dataset (lower FID is desired) Our FID results are based on estimates for the same reason as mentioned above.
    Looking at the Results in~\cref{fig:ddpm:fid}
    We notice that the FID score curves are approximately just the inverse of the Inception-score curves. As before we notice that the SDE method is an outlier, the fid score even seems to rise over epochs, suggesting that the SDE method is overfitting.
    As for the other methods the results are similar to the conditional method performs worst, and that the LDS method performs the best, suggesting that the conditional method is more prone to overfitting.