\subsection{A.1: Variations}
\subsubsection{Variation 0: Baseline}

\subsubsection{Variation 1: Low-discrepency sampler}
Here we implement a discretised version of the low-discrepency sampler (LDS) from \cite{kingma2023variationaldiffusionmodels}.

The baseline loss function provided to us works by independently sampling a timestep $t$ for each image in the batch of size $k$.
 
The LDS loss function works by sampling a single offset $u_0 \sim \mathcal{U}[0,1]$.
Each timestep $t_i$ is then computed as $t_i = mod(u_0 + i/k, 1)$. Here 0 is the first timestep and 1 is the last timestep in the continuous definition of the DDPM.

To discretise this process, we multiply these values by $T$ to map from $[0,1]$ to $[0, T]$. Where $T$ is the total number of diffusions performed by the model, then round to the nearest integer.
We note that rounding is equivalent to taking the floor of the value offset by $1/2$.

\begin{align}
  t_i &= \lfloor mod(u_0 + i/k, 1) \times T + \frac{1}{2} \rfloor, i = \{1, ..., k\}
\intertext{}
\end{align}


\subsubsection{Variation 2: Importance sampling}


\subsubsection{Variation 3: Classifier-free guidance}
Another variation we implemented is classifier-free guidance.
The motivation behind this method is two-fold:
\begin{enumerate}
  \item It allows us to generate specific images by providing additional guidance, in our case, the digit number.
  \item It allows the model to learn the conditional distributions, leading to better quality but less variability.
\end{enumerate}



To implement classifier-free guidance, we create a linear combination of a conditional and unconditional score estimator, as derived in \cite{classifier_free_guidance}, for our data which is conditioned on the labels
$y$.
\begin{align}
  \hat{\epsilon_\theta}(x_t, t, y) &= \epsilon_\theta(x_t, t, y) + w (\epsilon_\theta(x_t, t, y) - \epsilon_\theta(x_t, t))\label{cfg_epsilon}
\end{align}
where $w$ is a weighting factor that determines how strongly guidance is applied, at 0 it is unconditional, and as $w$ increases, the model becomes more guided,
with 1 leading to a conditional score estimator and $w > 1$ leading to increasingly higher fidelity and less diversity.
This estimator can be derived by looking at the conditional score $\nabla_{x_t} \log (p(y \mid x_t))$
and by applying Bayes' rule:
\begin{align}
  \nabla_{x_t} \log (p(y|x_t)) &= \nabla_{x_t} \log (p(x_t \mid y)) - \nabla_{x_t} \log (p(x_t)) + \nabla_{x_t} \log (p(y))\\
  \intertext{The last term is dropped since $y$ is constant with respect to $x_t$.}
  &= \nabla_{x_t} \log (p(x_t \mid y)) - \nabla_{x_t} \log (p(x_t))
\end{align}
With further derivation (using the equivalence between $\nabla_{x_t} \log (p(x_t))$ and $\epsilon_\theta(x_t, t)$, and the equation derived for classifier guided diffusion \cite{classifier_free_guidance}), the resulting equation \ref{cfg_epsilon} is reached. 

To implement this, we simply replace the original noise term $\epsilon_\theta(x_t, t)$ with the new term $\hat{\epsilon_\theta}(x_t, t, y)$.
We inject the class information into this new model $\hat{\epsilon_\theta}(x_t, t, y)$, giving us 
the conditioned model.
In the same way as we inject the time information into the model, we also inject the class information by adding a class embedding
which is analogous to the time embedding.
To get our unconditioned model, we mask 20\% of the class labels in training at random.
The masked value in our specific implementation is set to $10$, since 0 throgh 9 are 
the classes in the MNIST dataset.
We therefore get the unconditioned model $\epsilon_\theta(x_t, t) = \epsilon_\theta(x_t, t, y=10)$.

This method of dropping out the class information at random gives us a model
that both learns the conditional distributions of each digit, as well as the
general unconditional distribution of the whole dataset.

\subsubsection{Variation 4: Continuous-time diffusion model using SDEs}
%We also have form \cite{anderson_reverse-time_1982} that any SDE has a reverse time SDE of the form:
%\begin{equation}
%  \frac{d x} =  [f(x, t)-g^2 (t) \nabla_x \log p_t (x)]dt + g(t)dW
%\end{equation}

This variation uses a stochastic differential equation to model the forward and reverse diffusion process. 
Our forward process is a variance exploding SDE of form:
\begin{equation}
  dx = \sigma ^ t dW, t \in [0, 1]
\end{equation}
Which gives us the weighting of
$\lambda(t) = \frac{1}{2 \log \sigma} (\sigma^{2t}-1)$ as suggested by \cite{yang_song}.
This function is chosen to be inversely proportional to $\mathbb{E}[||\nabla_x \log p_{0t}(x(t) | x(0))||^2_2]$.
This function was then used to replace the weighting of our score-net
which was set to 1 for all $t \in [0, 1]$ for the three other variations (since they predict the noise instead of the score).
Our loss function was defined to be the Fisher divergence between $s_\theta(x_t, t)$ and $\nabla_x \log p_{\text{data}}(x)$.
Since the score-net is normalised according to the 
inversely proportional marginal distribution, we can omit the second term and we get our loss function:
\begin{align}
  \mathcal{L}_\text{SDE}(\theta) &= \mathbb{E}_{x(0) \sim p_\text{data}} \mathbb{E}_{t \sim \mathcal{U}[0, 1]} \left[ 
  \lambda(t) \mathbb{E}_{x(t) \sim q_{\sigma_t}(x(t) | x(0))} \left[
    ||s_\theta (x_t, t) - \nabla_{x(t)} (\log q(x(t) | x(0)))||^2_2
    \right]\right]\\
    &\approx \mathbb{E}_{x(0) \sim p_\text{data}} \mathbb{E}_{t \sim \mathcal{U}[0, 1]} \left[ 
  \lambda(t) \mathbb{E}_{x(t) \sim q_{\sigma_t}(x(t) | x(0))} \left[
    \left|\left|\frac{s_\theta(x_t, t)}{\lambda(t)}\right|\right|^2_2
    \right]\right]
\end{align}
We then train the model and perform our sampling using Euler-Maruyama sampling.

\subsection{A.2 Comparison}

\subsubsection{Visual Comparison}



Similar results overall, but we notice that ...



\subsubsection{Quatitative Comparison}
