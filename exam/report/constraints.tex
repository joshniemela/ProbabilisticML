\subsection{Fitting a standard GP}

\subsection{Learning with Integral Constraints}

We write the constraint $\hat{q}$ in matrix form:

\begin{align*}
  \hat{q} = \sum_{i=1}^\ell w_i f(x_i) = {w} f
\end{align*}
where $w = \begin{bmatrix} w_1, \ldots, w_\ell \end{bmatrix}$
and $f = \tp{\begin{bmatrix} f(x_1), \ldots, f(x_\ell) \end{bmatrix}}$.
%
Writing the joint distribution of $(\hat{q}, f)$ as a matrix
and facotring out $f$ we get
\begin{align*}
  (\hat{q}, f)
  = \left[ \begin{array}{c} \hat{q} \\ \hline f \end{array} \right]
  = \left[ \begin{array}{c} w \\ \hline I \end{array} \right] f
  = Q f
\end{align*}
with $Q = \left[ \begin{array}{c} w \\ \hline I \end{array} \right]$.

As $f \sim \mathcal{GP}(0, k(\cdot, \cdot))$, then we must by definition have
$f | X \sim \mathcal{N}\left( 0, K(X) \right)$
and as multivariate normal distributions are closed under linear transformations
we therefore have
\begin{align*}
  (\hat{q}, f) | X
  = Q f | X
  \sim \mathcal{N}\left( 0, Q K(X) \tp{Q} \right).
\end{align*}

Letting ${\Sigma} = Q K(X) \tp{Q}$, we partition ${\Sigma}$
into four blocks
\begin{align*}
  {\Sigma} &=
  \left[
    \begin{array}{c|c}
      \Sigma_{1 1} & \Sigma_{1 2} \\
      \hline
      \Sigma_{2 1} & \Sigma_{2 2} \\
    \end{array}
  \right]
  \intertext{
    where
  }
  \Sigma_{1 1} = w K(X) \tp{w},
  \quad
  \Sigma_{1 2} &= w K(X),
  \quad
  \Sigma_{2 1} = K(X) \tp{w},
  \quad
  \Sigma_{2 2} = K(X).
\end{align*}
We then have that the conditional
$f | X, \hat{q} \sim \mathcal{N}(\mu_{2|1}, \Sigma_{2|1})$, where
\begin{align*}
  \mu_{2|1} = \Sigma_{2 1} \Sigma_{1 1}^{-1} \hat{q},
  \quad
  \Sigma_{2|1} = \Sigma_{2 2} - \Sigma_{2 1} \Sigma_{1 1}^{-1} \tp{\Sigma_{2 1}}
\end{align*}
